{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def scrape_page(i):\n",
    "  page_reviews = []\n",
    "  time.sleep(2)\n",
    "  response = requests.get(f\"{url}?page={i}\")\n",
    "  web_page = response.text\n",
    "  soup = BeautifulSoup(web_page, \"html.parser\")\n",
    "  if soup=='<html><body>We have received an unusually large amount of requests from your IP so you have been rate limited</body></html>':\n",
    "    print(\"request limit\")\n",
    "\n",
    "  for review in soup.find_all(class_ = \"styles_reviewCardInner__EwDq2\"):\n",
    "      review_title = review.find(class_ = \"typography_heading-s__f7029 typography_appearance-default__AAY17\").getText()\n",
    "      review_date = review.select_one(selector=\"time\").getText()\n",
    "      review_rating = review.find(class_ = \"star-rating_starRating__4rrcf star-rating_medium__iN6Ty\").findChild()[\"alt\"]\n",
    "      review_text = review.find(class_ = \"typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn\")\n",
    "      if review_text == None:\n",
    "          review_text = \"\"\n",
    "      else:\n",
    "          review_text = review_text.get_text(separator=' ')\n",
    "      page_reviews.append((review_title, review_date, review_rating, review_text))\n",
    "\n",
    "  return page_reviews\n",
    "\n",
    "def parallel_scrapping(url):\n",
    "  soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "  if soup=='<html><body>We have received an unusually large amount of requests from your IP so you have been rate limited</body></html>':\n",
    "    print(\"request limit\")\n",
    "  page_numbers = []\n",
    "  for a in soup.find('div', class_= \"styles_pagination__6VmQv\").find_all('a', href=True):\n",
    "      if(a['name'].startswith('pagination-button-') and a.text.isnumeric()):\n",
    "          page_numbers.append(int(a.text))\n",
    "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "      results = list(executor.map(scrape_page, range(1, max(page_numbers) + 1)))\n",
    "  flat_results = [item for sublist in results for item in sublist]\n",
    "\n",
    "  return pd.DataFrame(flat_results, columns =['review_title', 'review_date', 'review_rating', 'review_text'])\n",
    "\n",
    "def get_links(url):\n",
    "    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "    page_numbers = []\n",
    "    for a in soup.find('div', class_= \"styles_paginationWrapper__fukEb styles_pagination__USObu\").find_all('a', href=True):\n",
    "      if(a['name'].startswith('pagination-button-') and a.text.isnumeric()):\n",
    "          page_numbers.append(int(a.text))\n",
    "\n",
    "    divs = soup.find_all('div', class_= \"paper_paper__1PY90 paper_outline__lwsUX card_card__lQWDv card_noPadding__D8PcU styles_wrapper__2JOo2\")\n",
    "\n",
    "    links = []\n",
    "\n",
    "    for i in range(1, max(page_numbers) + 1):\n",
    "      time.sleep(2)\n",
    "      response = requests.get(f\"{url}?page={i}\")\n",
    "      web_page = response.text\n",
    "      soup = BeautifulSoup(web_page, \"html.parser\")\n",
    "      if soup=='<html><body>We have received an unusually large amount of requests from your IP so you have been rate limited</body></html>':\n",
    "        print(\"request limit\")\n",
    "      for div in divs:\n",
    "          anchors = div.find_all('a', href=True)\n",
    "          for a in anchors:\n",
    "              href = a['href']\n",
    "              if href.startswith(\"/review/\"):\n",
    "                  links.append(href)\n",
    "\n",
    "    return [\"https://www.trustpilot.com\" + link for link in links]\n",
    "\n",
    "def process_links(url_list):\n",
    "    df_list = []\n",
    "    for url in url_list:\n",
    "        df = parallel_scrapping(url)\n",
    "        df['URL'] = url\n",
    "        df_list.append(df)\n",
    "    result = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links for travel insurance company\n",
    "url = 'https://www.trustpilot.com/categories/travel_insurance_company'\n",
    "url_list = get_links(url)\n",
    "df = process_links(url_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
